{
  "cells": [
    {
      "metadata": {
        "id": "cf4d8c9b006ada45"
      },
      "cell_type": "markdown",
      "source": [
        "## <center>CSE 546: Reinforcement Learning</center>\n",
        "### <center>Prof. Alina Vereshchaka</center>\n",
        "#### <center>Spring 2025</center>\n",
        "\n",
        "Welcome to the Assignment 3, Part 1: Introduction to Actor-Critic Methods! It includes the implementation of simple actor and critic networks and best practices used in modern Actor-Critic algorithms."
      ],
      "id": "cf4d8c9b006ada45"
    },
    {
      "metadata": {
        "id": "9d7a6d891e2fb312"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 0: Setup and Imports"
      ],
      "id": "9d7a6d891e2fb312"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53473293aa9daf8e",
        "outputId": "119c6c6b-4d88-4973-9d54-7a00d75643de"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "id": "53473293aa9daf8e",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79ffed0c1af0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import ale_py\n",
        "import traceback\n",
        "import warnings\n",
        "from gymnasium.spaces import Box, Discrete, MultiDiscrete\n",
        "\n",
        "try:\n",
        "    from gymnasium.wrappers import AtariPreprocessing\n",
        "except ImportError:\n",
        "    class AtariPreprocessing:\n",
        "        def __init__(self, *args, **kwargs): raise NotImplementedError(\"AtariPreprocessing Import Failed\")\n",
        "\n",
        "try:\n",
        "    # from gymnasium.wrappers import FrameStack\n",
        "    # from gym.wrappers import FrameStack\n",
        "    from gymnasium.wrappers import FrameStackObservation\n",
        "\n",
        "except ImportError:\n",
        "    class FrameStack:\n",
        "         def __init__(self, *args, **kwargs): raise NotImplementedError(\"Dummy FrameStack: Import Failed\")\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*WARN: Box observation space.*\")"
      ],
      "metadata": {
        "id": "fBNudmUw2Rue"
      },
      "id": "fBNudmUw2Rue",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.wrappers import FrameStackObservation\n"
      ],
      "metadata": {
        "id": "IBF4xFYWSTp4"
      },
      "id": "IBF4xFYWSTp4",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0qkzKu-2R1X",
        "outputId": "7c940e46-d215-4a32-c224-64c44a1abfa7"
      },
      "id": "l0qkzKu-2R1X",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ManualOneHotObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        if not isinstance(env.observation_space, Discrete):\n",
        "            raise ValueError(\"ManualOneHotObservation requires Discrete observation space.\")\n",
        "        self.n = env.observation_space.n\n",
        "        self.observation_space = Box(0.0, 1.0, (self.n,), dtype=np.float32)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        one_hot = np.zeros(self.n, dtype=np.float32)\n",
        "        one_hot[obs] = 1.0\n",
        "        return one_hot"
      ],
      "metadata": {
        "id": "XcfMez1e5AkQ"
      },
      "id": "XcfMez1e5AkQ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2a3d9c34ff222994"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 1: Actor-Critic Network Architectures and Loss Computation\n",
        "\n",
        "In this section, you will explore two common architectural designs for Actor-Critic methods and implement their corresponding loss functions using dummy tensors. These architectures are:\n",
        "- A. Completely separate actor and critic networks\n",
        "- B. A shared network with two output heads\n",
        "\n",
        "Both designs are widely used in practice. Shared networks are often more efficient and generalize better, while separate networks offer more control and flexibility.\n",
        "\n",
        "---\n"
      ],
      "id": "2a3d9c34ff222994"
    },
    {
      "metadata": {
        "id": "971fa7887dd4f858"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1a – Separate Actor and Critic Networks with Loss Function\n",
        "\n",
        "Define a class `SeparateActorCritic`. Your goal is to:\n",
        "- Create two completely independent neural networks: one for the actor and one for the critic.\n",
        "- The actor should output a probability distribution over discrete actions (use `nn.Softmax`).\n",
        "- The critic should output a single scalar value.\n",
        "\n",
        " Use `nn.ReLU()` as your activation function. Include at least one hidden layer of reasonable width (e.g. 64 or 128 units).\n",
        "\n",
        "```python\n",
        "# TODO: Define SeparateActorCritic class\n",
        "```\n",
        "\n",
        " Next, simulate training using dummy tensors:\n",
        "1. Generate dummy tensors for log-probabilities, returns, estimated values, and entropies.\n",
        "2. Compute the actor loss using the advantage (return - value).\n",
        "3. Compute the critic loss as mean squared error between values and returns.\n",
        "4. Use a single optimizer for both the Actor and the Critic. In this case, combine the actor and critic losses into a total loss and perform backpropagation.\n",
        "5. Use a separate optimizers for both the Actor and the Critic. In this case, keep the actor and critic losses separate and perform backpropagation.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate loss computation and backpropagation\n",
        "```\n",
        "\n",
        "🔗 Helpful references:\n",
        "- PyTorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
        "- PyTorch MSE Loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
        "\n",
        "---"
      ],
      "id": "971fa7887dd4f858"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd6b81ed1791e4e6",
        "outputId": "23134086-526f-4540-e7db-618e22c24367"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Define a class SeparateActorCritic with separate networks for actor and critic\n",
        "\n",
        "# BEGIN_YOUR_CODE\n",
        "class SeparateActorCritic(nn.Module):\n",
        "    def __init__(self, observation_dim, action_dim, hidden_dim=64):\n",
        "        super(SeparateActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(observation_dim, hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        ).to(device)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(observation_dim, hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = state.float().to(device)\n",
        "        action_logits = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        return action_logits, value\n",
        "\n",
        "## Simulating Loss adn BackProp\n",
        "N, obs_dim, act_dim = 10, 8, 4\n",
        "model = SeparateActorCritic(obs_dim, act_dim)\n",
        "\n",
        "dummy_states  = torch.randn(N, obs_dim)\n",
        "dummy_returns = torch.randn(N, 1)\n",
        "\n",
        "probs, values = model(dummy_states)\n",
        "print(f\"Return Values From Model:\\n Critic: {values} \\n Actor: {probs}\")\n",
        "dist        = torch.distributions.Categorical(probs)\n",
        "actions     = dist.sample()\n",
        "log_probs   = dist.log_prob(actions)\n",
        "\n",
        "advantage = (dummy_returns - values).detach()\n",
        "\n",
        "actor_loss  = -(log_probs * advantage.squeeze(-1)).mean()\n",
        "\n",
        "critic_loss = F.mse_loss(values, dummy_returns)\n",
        "\n",
        "opt_actor  = optim.Adam(model.actor.parameters(),  lr=1e-3)\n",
        "opt_critic = optim.Adam(model.critic.parameters(), lr=1e-3)\n",
        "\n",
        "opt_actor.zero_grad()\n",
        "opt_critic.zero_grad()\n",
        "\n",
        "actor_loss.backward()\n",
        "opt_actor.step()\n",
        "\n",
        "critic_loss.backward()\n",
        "opt_critic.step()\n",
        "\n",
        "print(f\"Calculated Actor Loss: {actor_loss.item()} And Critic Loss={critic_loss.item()}\")\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "dd6b81ed1791e4e6",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return Values From Model:\n",
            " Critic: tensor([[ 0.0032],\n",
            "        [ 0.0522],\n",
            "        [-0.0357],\n",
            "        [ 0.0978],\n",
            "        [-0.0136],\n",
            "        [ 0.1160],\n",
            "        [ 0.0752],\n",
            "        [ 0.0782],\n",
            "        [-0.0077],\n",
            "        [ 0.1205]], grad_fn=<AddmmBackward0>) \n",
            " Actor: tensor([[0.2906, 0.2723, 0.2130, 0.2241],\n",
            "        [0.2811, 0.2716, 0.2244, 0.2229],\n",
            "        [0.2922, 0.2914, 0.2207, 0.1957],\n",
            "        [0.2713, 0.2909, 0.2226, 0.2152],\n",
            "        [0.2652, 0.3112, 0.2142, 0.2094],\n",
            "        [0.2924, 0.2619, 0.2123, 0.2334],\n",
            "        [0.2729, 0.2987, 0.2224, 0.2061],\n",
            "        [0.2781, 0.2907, 0.2019, 0.2293],\n",
            "        [0.2953, 0.2820, 0.2160, 0.2068],\n",
            "        [0.2874, 0.2806, 0.2235, 0.2085]], grad_fn=<SoftmaxBackward0>)\n",
            "Calculated Actor Loss: -0.39679527282714844 And Critic Loss=0.8613286018371582\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "metadata": {
        "id": "eb8e90c88108cd2e"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:\n",
        "\n",
        "So here we are using a seprate netwroks for both actor and critic. In our opinion this setup gives us the advantage of being more simple and stable.\n",
        "Moreover, because both critic and the actor have a different objective to full-fill they might benefit more from having different gradients then allowing for shared learning."
      ],
      "id": "eb8e90c88108cd2e"
    },
    {
      "metadata": {
        "id": "64081a606b93029d"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1b – Shared Network with Actor and Critic Heads + Loss Function\n",
        "\n",
        "Now define a class `SharedActorCritic`:\n",
        "- Build a shared base network (e.g., linear layer + ReLU)\n",
        "- Create two heads: one for actor (output action probabilities) and one for critic (output state value)\n",
        "\n",
        "```python\n",
        "# TODO: Define SharedActorCritic class\n",
        "```\n",
        "\n",
        "Then:\n",
        "1. Pass a dummy input tensor through the model to obtain action probabilities and value.\n",
        "2. Simulate dummy rewards and compute advantage.\n",
        "3. Compute the actor and critic losses, combine them, and backpropagate.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate shared network loss computation and backpropagation\n",
        "```\n",
        "\n",
        " Use `nn.Softmax` for actor output and `nn.Linear` for scalar critic output.\n",
        "\n",
        "🔗 More reading:\n",
        "- Policy Gradient Methods: https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
        "- Actor-Critic Overview: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\n",
        "- PyTorch Categorical Distribution: https://pytorch.org/docs/stable/distributions.html#categorical\n",
        "\n",
        "---"
      ],
      "id": "64081a606b93029d"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a48f882fff11aecc",
        "outputId": "84cc2574-57c9-4c53-974d-95567d949098"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "class SharedActorCritic(nn.Module):\n",
        "    def __init__(self, observation_dim, action_dim, hidden_dim=64):\n",
        "        super(SharedActorCritic, self).__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(observation_dim, hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU()\n",
        "        ).to(device)\n",
        "        # self.actor_head = nn.Linear(hidden_dim, action_dim).to(device)\n",
        "        self.actor_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Softmax(dim=-1),\n",
        "        )\n",
        "        self.critic_head = nn.Linear(hidden_dim, 1).to(device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = state.float().to(device)\n",
        "        shared_features = self.shared(state)\n",
        "        action_logits = self.actor_head(shared_features)\n",
        "        value = self.critic_head(shared_features)\n",
        "        return action_logits, value\n",
        "\n",
        "obs_dim, act_dim = 8, 4\n",
        "model     = SharedActorCritic(obs_dim, act_dim).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "N = 10\n",
        "states  = torch.randn(N, obs_dim, device=device)\n",
        "returns = torch.randn(N, 1,    device=device)\n",
        "probs, values = model(states)\n",
        "print(f\"Return Values From Model:\\n Critic: {values} \\n Actor: {probs}\")\n",
        "\n",
        "dist      = torch.distributions.Categorical(probs)\n",
        "actions   = dist.sample()\n",
        "log_probs = dist.log_prob(actions)\n",
        "\n",
        "adv = (returns - values).detach()\n",
        "\n",
        "actor_loss  = -(log_probs * adv.squeeze(-1)).mean()\n",
        "critic_loss = F.mse_loss(values, returns)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "(actor_loss + critic_loss).backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Calculated Actor Loss: {actor_loss.item()} And Critic Loss={critic_loss.item()}\")\n",
        "# END_YOUR_CODE"
      ],
      "id": "a48f882fff11aecc",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return Values From Model:\n",
            " Critic: tensor([[-0.0331],\n",
            "        [-0.0798],\n",
            "        [-0.0686],\n",
            "        [-0.1056],\n",
            "        [ 0.0533],\n",
            "        [ 0.1209],\n",
            "        [ 0.0556],\n",
            "        [ 0.0528],\n",
            "        [-0.0608],\n",
            "        [ 0.0732]], grad_fn=<AddmmBackward0>) \n",
            " Actor: tensor([[0.2581, 0.2051, 0.2917, 0.2450],\n",
            "        [0.2634, 0.2092, 0.2887, 0.2387],\n",
            "        [0.2741, 0.2066, 0.2863, 0.2329],\n",
            "        [0.2517, 0.2140, 0.3234, 0.2110],\n",
            "        [0.2854, 0.2175, 0.2871, 0.2100],\n",
            "        [0.2761, 0.2280, 0.2852, 0.2107],\n",
            "        [0.2636, 0.2246, 0.2891, 0.2226],\n",
            "        [0.2697, 0.2272, 0.2737, 0.2294],\n",
            "        [0.2799, 0.2036, 0.2926, 0.2239],\n",
            "        [0.2518, 0.2314, 0.3108, 0.2060]], grad_fn=<SoftmaxBackward0>)\n",
            "Calculated Actor Loss: -0.288370817899704 And Critic Loss=1.382912516593933\n"
          ]
        }
      ],
      "execution_count": 28
    },
    {
      "metadata": {
        "id": "a974e302d1fdb028"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "a974e302d1fdb028"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are using a combined base network for actor and critic. The motivation behind this setup is that it might be better to learn shared features for both actor and the crtic. This is prefered as a shared setup is also less computationaly expensive and can also take less time to train.\n",
        "Moreover, in RL in some environments it might be that the featurs for understandign state's value are also correlated with calculating best actions."
      ],
      "metadata": {
        "id": "C7xC8PoIInw2"
      },
      "id": "C7xC8PoIInw2"
    },
    {
      "metadata": {
        "id": "eb645eb009b85b1c"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 2: Auto-Adaptive Network Setup for Environments\n",
        "\n",
        "You will now create a function that builds a shared actor-critic network that adapts to any Gymnasium environment. This function should inspect the environment and build input/output layers accordingly."
      ],
      "id": "eb645eb009b85b1c"
    },
    {
      "metadata": {
        "id": "4223b6ddf43abee5"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 2: Auto-generate Input and Output Layers\n",
        "Write a function `create_shared_network(env)` that constructs a neural network using the following rules:\n",
        "- The input layer should match the environment's observation space.\n",
        "- The output layer for the **actor** should depend on the action space:\n",
        "  - For discrete actions: output probabilities using `nn.Softmax`.\n",
        "  - For continuous actions: output mean and log std for a Gaussian distribution.\n",
        "- The **critic** always outputs a single scalar value.\n",
        "\n",
        "```python\n",
        "# TODO: Define function `create_shared_network(env)`\n",
        "```\n",
        "\n",
        "#### Environments to Support:\n",
        "Test your function with the following environments:\n",
        "1. `CliffWalking-v0` (Use one-hot encoding for discrete integer observations.)\n",
        "2. `LunarLander-v3` (Standard Box space for observations and discrete actions.)\n",
        "3. `PongNoFrameskip-v4` (Use gym wrappers for Atari image preprocessing.)\n",
        "4. `HalfCheetah-v5` (Continuous observation and continuous action.)\n",
        "\n",
        "```python\n",
        "# TODO: Loop through environments and test `create_shared_network`\n",
        "```\n",
        "\n",
        "Hint: Use `gym.spaces` utilities to determine observation/action types dynamically.\n",
        "\n",
        "🔗 Observation/Action Space Docs:\n",
        "- https://gymnasium.farama.org/api/spaces/\n",
        "\n",
        "---"
      ],
      "id": "4223b6ddf43abee5"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "d6d249ff9277403a",
        "outputId": "45d8d3b7-c63f-4fde-cd72-7dfd85d6bae6"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "# Task 2 auto generating actor critic networks based on environment spaces\n",
        "def create_shared_network(env):\n",
        "    obs_space = env.observation_space\n",
        "    action_space = env.action_space\n",
        "    print(\"Obs\", obs_space, len(obs_space.shape))\n",
        "    class DynamicActorCritic(nn.Module):\n",
        "        def __init__(self, obs_space, action_space):\n",
        "            super(DynamicActorCritic, self).__init__()\n",
        "            self.obs_space = obs_space\n",
        "            self.action_space = action_space\n",
        "            self._build_model()\n",
        "\n",
        "        def _build_model(self):\n",
        "            obs_shape = self.obs_space.shape\n",
        "            # handling different observation space types\n",
        "            if isinstance(self.obs_space, Discrete):\n",
        "                # using embeddings for discrete observations\n",
        "                self.embedding = nn.Embedding(self.obs_space.n, 16)\n",
        "                self.feature_extractor = nn.Sequential(nn.Linear(16, 64), nn.ReLU())\n",
        "                feature_size = 64\n",
        "            elif isinstance(self.obs_space, Box):\n",
        "                if len(obs_shape) == 1:\n",
        "                    # for vector observations (like lunarlander)\n",
        "                    obs_dim = obs_shape[0]\n",
        "                    hidden_dim_mlp = 64\n",
        "                    self.feature_extractor = nn.Sequential(\n",
        "                        nn.Linear(obs_dim, hidden_dim_mlp), nn.ReLU(),\n",
        "                        nn.Linear(hidden_dim_mlp, hidden_dim_mlp), nn.ReLU()\n",
        "                    )\n",
        "                    feature_size = hidden_dim_mlp\n",
        "                elif len(obs_shape) == 2:\n",
        "                    # for 2d observations (like preprocessed atari frames)\n",
        "                    in_channels = 1\n",
        "                    H, W = obs_shape\n",
        "                    self.cnn_base = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels, 32, 8, 4), nn.ReLU(),\n",
        "                        nn.Conv2d(32, 64, 4, 2), nn.ReLU(),\n",
        "                        nn.Conv2d(64, 64, 3, 1), nn.ReLU(), nn.Flatten()\n",
        "                    )\n",
        "                    with torch.no_grad():\n",
        "                        dummy_input = torch.zeros(1, in_channels, H, W).to(device)\n",
        "                        cnn_output_size = self.cnn_base(dummy_input).shape[1]\n",
        "                    fc_hidden_dim = 512\n",
        "                    self.feature_extractor = nn.Sequential(\n",
        "                        self.cnn_base, nn.Linear(cnn_output_size, fc_hidden_dim), nn.ReLU()\n",
        "                    )\n",
        "                    feature_size = fc_hidden_dim\n",
        "                elif len(obs_shape) == 3:\n",
        "                    # for 3d observations (like stacked frames)\n",
        "                    in_channels = obs_shape[0]\n",
        "                    self.cnn_base = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels, 32, 8, 4), nn.ReLU(),\n",
        "                        nn.Conv2d(32, 64, 4, 2), nn.ReLU(),\n",
        "                        nn.Conv2d(64, 64, 3, 1), nn.ReLU(), nn.Flatten()\n",
        "                    )\n",
        "                    with torch.no_grad():\n",
        "                        dummy_input = torch.zeros(1, *obs_shape).to(device)\n",
        "                        cnn_output_size = self.cnn_base(dummy_input).shape[1]\n",
        "                    fc_hidden_dim = 512\n",
        "                    self.feature_extractor = nn.Sequential(\n",
        "                        self.cnn_base, nn.Linear(cnn_output_size, fc_hidden_dim), nn.ReLU()\n",
        "                    )\n",
        "                    feature_size = fc_hidden_dim\n",
        "                else: raise NotImplementedError(f\"Unsupported Box obs shape: {obs_shape}\")\n",
        "            else: raise NotImplementedError(f\"Unsupported observation space: {type(self.obs_space)}\")\n",
        "\n",
        "            # shared layers that both actor and critic will use\n",
        "            shared_layer_out_dim = 64\n",
        "            self.shared_layer = nn.Sequential(\n",
        "                nn.Linear(feature_size, shared_layer_out_dim), nn.ReLU()\n",
        "            )\n",
        "            final_feature_size = shared_layer_out_dim\n",
        "\n",
        "            # handling different action space types\n",
        "            if isinstance(self.action_space, Discrete):\n",
        "                # for discrete actions (like cliffwalking, pong)\n",
        "                # self.actor_head = nn.Linear(final_feature_size, )\n",
        "                self.actor_head = nn.Sequential(\n",
        "                    nn.Linear(final_feature_size, self.action_space.n),\n",
        "                    nn.Softmax(dim=-1)\n",
        "                )\n",
        "                self._action_adapter = self._discrete_action_adapter\n",
        "            elif isinstance(self.action_space, Box):\n",
        "                # for continuous actions (like halfcheetah)\n",
        "                action_dim = self.action_space.shape[0]\n",
        "                self.actor_mean = nn.Linear(final_feature_size, action_dim)\n",
        "                self.actor_logstd = nn.Parameter(torch.zeros(action_dim))\n",
        "                self._action_adapter = self._continuous_action_adapter\n",
        "            elif isinstance(self.action_space, MultiDiscrete):\n",
        "                # for multi discrete actions\n",
        "                self.nvec = self.action_space.nvec\n",
        "                total_action_dim = int(np.sum(self.nvec))\n",
        "                self.actor_head = nn.Linear(final_feature_size, total_action_dim)\n",
        "                self._action_adapter = self._multidiscrete_action_adapter\n",
        "            else: raise NotImplementedError(f\"Unsupported action space: {type(self.action_space)}\")\n",
        "\n",
        "            # value function head is always a single output\n",
        "            self.critic_head = nn.Linear(final_feature_size, 1)\n",
        "\n",
        "        # action adapters for different action space types\n",
        "        def _discrete_action_adapter(self, features): return self.actor_head(features)\n",
        "\n",
        "        def _continuous_action_adapter(self, features):\n",
        "            mean = self.actor_mean(features)\n",
        "            logstd = self.actor_logstd.expand_as(mean)\n",
        "            return (mean, logstd)\n",
        "\n",
        "        def _multidiscrete_action_adapter(self, features):\n",
        "            logits_concat = self.actor_head(features)\n",
        "            chunks = torch.split(logits_concat, self.nvec.tolist(), dim=1)\n",
        "            probs = [F.softmax(chunk, dim=-1) for chunk in chunks]\n",
        "            return probs\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.float().to(device)\n",
        "            obs_shape = self.obs_space.shape\n",
        "            # handling preprocessing based on observation type\n",
        "            if isinstance(self.obs_space, Discrete):\n",
        "                x = self.embedding(x.long().squeeze(-1).to(device))\n",
        "            elif isinstance(self.obs_space, Box) and len(obs_shape) == 2:\n",
        "                 x = x.unsqueeze(1)\n",
        "\n",
        "            # forward pass through shared layers\n",
        "            features = self.feature_extractor(x)\n",
        "            shared_features = self.shared_layer(features)\n",
        "\n",
        "            # getting action and value outputs\n",
        "            action_outputs = self._action_adapter(shared_features)\n",
        "            value = self.critic_head(shared_features)\n",
        "            return action_outputs, value\n",
        "\n",
        "    model = DynamicActorCritic(obs_space, action_space).to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Testing ALL ENVIRONEMNTS:-\n",
        "\n",
        "print(\"\\n Running Sequential Test Execution \")\n",
        "\n",
        "# testing the implementation on different environments\n",
        "environments_to_run = [\n",
        "    \"CliffWalking-v0\",\n",
        "    \"LunarLander-v3\",\n",
        "    \"PongNoFrameskip-v4\",\n",
        "    \"HalfCheetah-v5\"\n",
        "]\n",
        "\n",
        "for env_name in environments_to_run:\n",
        "    print(f\"\\n Testing Environment: {env_name} \")\n",
        "    env = None\n",
        "    model = None\n",
        "    skip_tests = False\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n Setting up: {env_name} \")\n",
        "        if env_name == \"CliffWalking-v0\":\n",
        "            # for discrete state space, applying one hot encoding\n",
        "            base_env = gym.make(env_name, render_mode=None)\n",
        "            env = ManualOneHotObservation(base_env)\n",
        "        elif env_name == \"PongNoFrameskip-v4\":\n",
        "            base_env = gym.make(env_name, obs_type=\"grayscale\", render_mode=None)\n",
        "            env = AtariPreprocessing(base_env, screen_size=84, grayscale_obs=True, scale_obs=True, terminal_on_life_loss=False)\n",
        "            env = FrameStackObservation(env, 4)\n",
        "        elif env_name == \"HalfCheetah-v5\":\n",
        "            # mujoco environment with continuous action space\n",
        "            env = gym.make(env_name, render_mode=None)\n",
        "        elif env_name == \"LunarLander-v3\":\n",
        "             env = gym.make(env_name, render_mode=None)\n",
        "        else:\n",
        "            print(f\"Warning: Environment '{env_name}' not explicitly handled.\")\n",
        "            skip_tests = True\n",
        "\n",
        "        if env is None: raise ValueError(\"Environment creation failed.\")\n",
        "\n",
        "    except gym.error.DependencyNotInstalled as e:\n",
        "        print(f\"SKIPPING {env_name}: Missing Dependencies. {e}\")\n",
        "        skip_tests = True\n",
        "    except NotImplementedError as e:\n",
        "         print(f\"SKIPPING {env_name}: A required wrapper failed ({e})\")\n",
        "         skip_tests = True\n",
        "    except Exception as e:\n",
        "         print(f\"ERROR during {env_name} setup: {e}\")\n",
        "         print(traceback.format_exc())\n",
        "         skip_tests = True\n",
        "\n",
        "    if not skip_tests and env is not None:\n",
        "        try:\n",
        "            print(f\"\\nRunning tests for: {env_name}\")\n",
        "            # creating a network based on environment specs\n",
        "            model = create_shared_network(env)\n",
        "            obs, _ = env.reset(seed=SEED)\n",
        "            obs_np = np.array(obs)\n",
        "            # testing forward pass through the network\n",
        "            print(\"\\nTesting Forward Pass \")\n",
        "            obs_tensor = torch.tensor(obs_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            model.eval()\n",
        "            with torch.no_grad(): action_output, value_output = model(obs_tensor)\n",
        "            model.train()\n",
        "            print(\"Forward pass successful.\")\n",
        "\n",
        "            optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "            action_output_grad, value_output_grad = model(obs_tensor)\n",
        "            loss_actor = torch.tensor(0.0, device=device)\n",
        "            loss_critic = torch.tensor(0.0, device=device)\n",
        "            dummy_return = torch.randn(1, 1, device=device)\n",
        "            loss_critic = F.mse_loss(value_output_grad, dummy_return)\n",
        "\n",
        "            # handling different action space types for loss calculation\n",
        "            if isinstance(env.action_space, Discrete):\n",
        "                logits = torch.log(action_output_grad + 1e-8)\n",
        "                loss_actor = F.cross_entropy(logits, torch.randint(0, env.action_space.n, (1,), device=device))\n",
        "            elif isinstance(env.action_space, Box):\n",
        "                mean, logstd = action_output_grad\n",
        "                std = torch.exp(logstd).clamp(min=1e-6)\n",
        "                dist = torch.distributions.Normal(mean, std)\n",
        "                log_prob = dist.log_prob(torch.randn_like(mean)).sum(axis=-1, keepdim=True)\n",
        "                loss_actor = -log_prob.mean()\n",
        "            elif isinstance(env.action_space, MultiDiscrete):\n",
        "                 loss_actor_total = 0\n",
        "                 for i, logits in enumerate(action_output_grad):\n",
        "                     logits = torch.log(logits + 1e-8)\n",
        "                     loss_actor_total += F.cross_entropy(logits, torch.randint(0, env.action_space.nvec[i], (1,), device=device))\n",
        "                 loss_actor = loss_actor_total\n",
        "\n",
        "            total_loss = loss_actor + loss_critic\n",
        "            print(f\"Calculated Dummy Loss For Env ({env_name}): {total_loss.item():.4f}\")\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "            optimizer.step()\n",
        "            print(\"==== DONE WITH BACKPROP ===\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n!!! Error during testing execution for {env_name}: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "        finally:\n",
        "            if env is not None: env.close()\n",
        "\n",
        "print(\"\\n All Environment Tests Completed \")\n",
        "\n",
        "\n",
        "\"\"\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "d6d249ff9277403a",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Running Sequential Test Execution \n",
            "\n",
            " Testing Environment: CliffWalking-v0 \n",
            "\n",
            " Setting up: CliffWalking-v0 \n",
            "\n",
            "Running tests for: CliffWalking-v0\n",
            "Obs Box(0.0, 1.0, (48,), float32) 1\n",
            "\n",
            "Testing Forward Pass \n",
            "Forward pass successful.\n",
            "Calculated Dummy Loss For Env (CliffWalking-v0): 1.8187\n",
            "==== DONE WITH BACKPROP ===\n",
            "\n",
            " Testing Environment: LunarLander-v3 \n",
            "\n",
            " Setting up: LunarLander-v3 \n",
            "\n",
            "Running tests for: LunarLander-v3\n",
            "Obs Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32) 1\n",
            "\n",
            "Testing Forward Pass \n",
            "Forward pass successful.\n",
            "Calculated Dummy Loss For Env (LunarLander-v3): 2.9827\n",
            "==== DONE WITH BACKPROP ===\n",
            "\n",
            " Testing Environment: PongNoFrameskip-v4 \n",
            "\n",
            " Setting up: PongNoFrameskip-v4 \n",
            "\n",
            "Running tests for: PongNoFrameskip-v4\n",
            "Obs Box(0.0, 1.0, (4, 84, 84), float32) 3\n",
            "\n",
            "Testing Forward Pass \n",
            "Forward pass successful.\n",
            "Calculated Dummy Loss For Env (PongNoFrameskip-v4): 2.3838\n",
            "==== DONE WITH BACKPROP ===\n",
            "\n",
            " Testing Environment: HalfCheetah-v5 \n",
            "\n",
            " Setting up: HalfCheetah-v5 \n",
            "\n",
            "Running tests for: HalfCheetah-v5\n",
            "Obs Box(-inf, inf, (17,), float64) 1\n",
            "\n",
            "Testing Forward Pass \n",
            "Forward pass successful.\n",
            "Calculated Dummy Loss For Env (HalfCheetah-v5): 9.2576\n",
            "==== DONE WITH BACKPROP ===\n",
            "\n",
            " All Environment Tests Completed \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "execution_count": 53
    },
    {
      "metadata": {
        "id": "4ccd13f0b62b30ff"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "4ccd13f0b62b30ff"
    },
    {
      "metadata": {
        "id": "ee2dd81024ce246a"
      },
      "cell_type": "code",
      "source": [],
      "id": "ee2dd81024ce246a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "b39c886fa536a639"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 3: Write Observation Normalization Function\n",
        "Create a function `normalize_observation(obs, env)` that:\n",
        "- Checks if the observation space is `Box` and has `low` and `high` attributes.\n",
        "- If so, normalize the input observation.\n",
        "- Otherwise, return the observation unchanged.\n",
        "\n",
        "```python\n",
        "# TODO: Define `normalize_observation(obs, env)`\n",
        "```\n",
        "\n",
        "Test this function with observations from:\n",
        "- `LunarLander-v3`\n",
        "- `PongNoFrameskip-v4`\n",
        "\n",
        "Note: Atari observations are image arrays. Normalize pixel values to [0, 1]. For LunarLander-v3, the different elements in the observation vector have different ranges. Normalize them to [0, 1] using the `low` and `high` attributes of the observation space.\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "b39c886fa536a639"
    },
    {
      "metadata": {
        "id": "fc7ee06112cf7d29"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "def normalize_observation(obs: np.ndarray, env: gym.Env) -> np.ndarray:\n",
        "    obs_space = env.observation_space\n",
        "    if not isinstance(obs_space, Box): return obs.astype(np.float32)\n",
        "\n",
        "    low = obs_space.low\n",
        "    high = obs_space.high\n",
        "\n",
        "    if not (np.all(np.isfinite(low)) and np.all(np.isfinite(high))):\n",
        "        return obs.astype(np.float32)\n",
        "\n",
        "    if np.any((high - low) <= 1e-8):\n",
        "        return obs.astype(np.float32)\n",
        "\n",
        "    denominator = high - low\n",
        "    normalized_obs = 2.0 * (obs.astype(np.float32) - low) / denominator - 1.0\n",
        "    return np.clip(normalized_obs, -1.0, 1.0).astype(np.float32)\n",
        "# END_YOUR_CODE"
      ],
      "id": "fc7ee06112cf7d29",
      "outputs": [],
      "execution_count": 7
    },
    {
      "metadata": {
        "id": "501ed2a6e7ca7a7b"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "501ed2a6e7ca7a7b"
    },
    {
      "metadata": {
        "id": "78211b617a843f62"
      },
      "cell_type": "code",
      "source": [],
      "id": "78211b617a843f62",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6b5fb5353307f514"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 4: Gradient Clipping\n",
        "\n",
        "To prevent exploding gradients, it's common practice to clip gradients before optimizer updates.\n",
        "\n",
        "### Task 4: Clip Gradients for Actor-Critic Networks\n",
        "Use dummy tensors and apply gradient clipping with the following PyTorch method:\n",
        "```python\n",
        "# During training, after loss.backward():\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "```\n",
        "\n",
        "Reuse the loss computation from Task 1a or 1b. After computing the gradients, apply gradient clipping.\n",
        "Print the gradient norm before and after clipping to verify it’s applied.\n",
        "\n",
        "🔗 PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "6b5fb5353307f514"
    },
    {
      "metadata": {
        "id": "7327507fb6e803ad"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "def apply_gradient_clipping(model: nn.Module, optimizer: optim.Optimizer, loss: torch.Tensor, max_norm: float = 1.0):\n",
        "    if not isinstance(loss, torch.Tensor) or not loss.requires_grad:\n",
        "        print(f\" Skipping gradient clipping: invalid loss.\")\n",
        "        return\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # printing gradient norm before clipping\n",
        "    total_norm_before = torch.nn.utils.clip_grad_norm_(\n",
        "        model.parameters(), max_norm=max_norm, norm_type=2.0, error_if_nonfinite=False\n",
        "    ).item()\n",
        "    print(f\"Gradient norm before clipping: {total_norm_before:.4f}\")\n",
        "\n",
        "    # calculating gradient norm after clipping as a validation check\n",
        "    total_norm_after = 0.0\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            if torch.isfinite(p.grad).all():\n",
        "                param_norm = p.grad.data.norm(2.0)\n",
        "                total_norm_after += param_norm.item() ** 2\n",
        "            else:\n",
        "                 print(f\" Warning: Non-finite gradients detected after clipping attempt.\")\n",
        "    total_norm_after = total_norm_after ** 0.5\n",
        "    print(f\"Gradient norm after clipping (manual check): {total_norm_after:.4f}\")\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "7327507fb6e803ad",
      "outputs": [],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "9952750fa74cd487"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "9952750fa74cd487"
    },
    {
      "metadata": {
        "id": "557a9303f5a1c863"
      },
      "cell_type": "code",
      "source": [],
      "id": "557a9303f5a1c863",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f4cff31e6c6e7e4a"
      },
      "cell_type": "markdown",
      "source": [
        "If you are working in a team, provide a contribution summary.\n",
        "| Team Member | Step# | Contribution (%) |\n",
        "|---|---|---|\n",
        "|   | Task 1 |   |\n",
        "|   | Task 2 |   |\n",
        "|   | Task 3 |   |\n",
        "|   | Task 4 |   |\n",
        "|   | **Total** |   |\n"
      ],
      "id": "f4cff31e6c6e7e4a"
    },
    {
      "metadata": {
        "id": "4be0a6e29f281e23"
      },
      "cell_type": "code",
      "source": [],
      "id": "4be0a6e29f281e23",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Running Sequential Test Execution \")\n",
        "\n",
        "# testing the implementation on different environments\n",
        "environments_to_run = [\n",
        "    \"CliffWalking-v0\",\n",
        "    \"LunarLander-v3\",\n",
        "    \"PongNoFrameskip-v4\",\n",
        "    \"HalfCheetah-v4\"\n",
        "]\n",
        "\n",
        "for env_name in environments_to_run:\n",
        "    print(f\"\\n Testing Environment: {env_name} \")\n",
        "    env = None\n",
        "    model = None\n",
        "    skip_tests = False\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n Setting up: {env_name} \")\n",
        "        if env_name == \"CliffWalking-v0\":\n",
        "            # for discrete state space, applying one hot encoding\n",
        "            base_env = gym.make(env_name, render_mode=None)\n",
        "            env = ManualOneHotObservation(base_env)\n",
        "        elif env_name == \"PongNoFrameskip-v4\":\n",
        "            is_framestack_dummy = False\n",
        "            try: _ = FrameStack(None, 0)\n",
        "            except NotImplementedError: is_framestack_dummy = True\n",
        "            except Exception: pass\n",
        "            # using ataripreprocessing wrapper to handle the image observations\n",
        "            base_env = gym.make(env_name, obs_type=\"grayscale\", render_mode=None)\n",
        "            env = AtariPreprocessing(base_env, screen_size=84, grayscale_obs=True, scale_obs=True, terminal_on_life_loss=False)\n",
        "            if not is_framestack_dummy:\n",
        "                 print(\"INFO: Skipping FrameStack application (unavailable/dummy).\")\n",
        "            else:\n",
        "                 print(\"INFO: Skipping FrameStack application (unavailable/dummy).\")\n",
        "        elif env_name == \"HalfCheetah-v4\":\n",
        "            # mujoco environment with continuous action space\n",
        "            env = gym.make(env_name, render_mode=None)\n",
        "        elif env_name == \"LunarLander-v3\":\n",
        "             env = gym.make(env_name, render_mode=None)\n",
        "        else:\n",
        "            print(f\"Warning: Environment '{env_name}' not explicitly handled.\")\n",
        "            skip_tests = True\n",
        "\n",
        "        if env is None: raise ValueError(\"Environment creation failed.\")\n",
        "\n",
        "    except gym.error.DependencyNotInstalled as e:\n",
        "        print(f\"SKIPPING {env_name}: Missing Dependencies. {e}\")\n",
        "        skip_tests = True\n",
        "    except NotImplementedError as e:\n",
        "         print(f\"SKIPPING {env_name}: A required wrapper failed ({e})\")\n",
        "         skip_tests = True\n",
        "    except Exception as e:\n",
        "         print(f\"ERROR during {env_name} setup: {e}\")\n",
        "         print(traceback.format_exc())\n",
        "         skip_tests = True\n",
        "\n",
        "    if not skip_tests and env is not None:\n",
        "        try:\n",
        "            print(f\"\\nRunning tests for: {env_name}\")\n",
        "            # creating a network based on environment specs\n",
        "            model = create_shared_network(env)\n",
        "\n",
        "            # testing observation normalization\n",
        "            print(\"\\nTesting Normalization \")\n",
        "            obs, _ = env.reset(seed=SEED)\n",
        "            obs_np = np.array(obs)\n",
        "            normalized_obs = normalize_observation(obs_np, env)\n",
        "            print(f\"Obs Range: Original=[{np.min(obs_np):.2f},{np.max(obs_np):.2f}] -> Normalized=[{np.min(normalized_obs):.2f},{np.max(normalized_obs):.2f}]\")\n",
        "\n",
        "            # testing forward pass through the network\n",
        "            print(\"\\nTesting Forward Pass \")\n",
        "            obs_tensor = torch.tensor(normalized_obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            model.eval()\n",
        "            with torch.no_grad(): action_output, value_output = model(obs_tensor)\n",
        "            model.train()\n",
        "            print(\"Forward pass successful.\")\n",
        "\n",
        "            # testing gradient clipping\n",
        "            print(\"\\nTesting Gradient Clipping \")\n",
        "            optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "            action_output_grad, value_output_grad = model(obs_tensor)\n",
        "            loss_actor = torch.tensor(0.0, device=device)\n",
        "            loss_critic = torch.tensor(0.0, device=device)\n",
        "            dummy_return = torch.randn(1, 1, device=device)\n",
        "            loss_critic = F.mse_loss(value_output_grad, dummy_return)\n",
        "\n",
        "            # handling different action space types for loss calculation\n",
        "            if isinstance(env.action_space, Discrete):\n",
        "                loss_actor = F.cross_entropy(action_output_grad, torch.randint(0, env.action_space.n, (1,), device=device))\n",
        "            elif isinstance(env.action_space, Box):\n",
        "                mean, logstd = action_output_grad\n",
        "                std = torch.exp(logstd).clamp(min=1e-6)\n",
        "                dist = torch.distributions.Normal(mean, std)\n",
        "                log_prob = dist.log_prob(torch.randn_like(mean)).sum(axis=-1, keepdim=True)\n",
        "                loss_actor = -log_prob.mean()\n",
        "            elif isinstance(env.action_space, MultiDiscrete):\n",
        "                 loss_actor_total = 0\n",
        "                 for i, logits in enumerate(action_output_grad):\n",
        "                     loss_actor_total += F.cross_entropy(logits, torch.randint(0, env.action_space.nvec[i], (1,), device=device))\n",
        "                 loss_actor = loss_actor_total\n",
        "\n",
        "            total_loss = loss_actor + loss_critic\n",
        "            print(f\"Calculated Dummy Loss: {total_loss.item():.4f}\")\n",
        "            apply_gradient_clipping(model, optimizer, total_loss, max_norm=0.5)\n",
        "            print(\"Gradient clipping applied.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n!!! Error during testing execution for {env_name}: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "        finally:\n",
        "            if env is not None: env.close()\n",
        "\n",
        "print(\"\\n All Environment Tests Completed \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUyyCC0f4poZ",
        "outputId": "f2f0ced7-0e42-461e-f5ab-1d9cd186d8d2"
      },
      "id": "uUyyCC0f4poZ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Running Sequential Test Execution \n",
            "\n",
            " Testing Environment: CliffWalking-v0 \n",
            "\n",
            " Setting up: CliffWalking-v0 \n",
            "\n",
            "Running tests for: CliffWalking-v0\n",
            "\n",
            "Testing Normalization \n",
            "Obs Range: Original=[0.00,1.00] -> Normalized=[-1.00,1.00]\n",
            "\n",
            "Testing Forward Pass \n",
            "Forward pass successful.\n",
            "\n",
            "Testing Gradient Clipping \n",
            "Calculated Dummy Loss: 1.3616\n",
            "Gradient norm before clipping: 1.2765\n",
            "Gradient norm after clipping (manual check): 0.5000\n",
            "Gradient clipping applied.\n",
            "\n",
            " Testing Environment: LunarLander-v3 \n",
            "\n",
            " Setting up: LunarLander-v3 \n",
            "\n",
            "Running tests for: LunarLander-v3\n",
            "\n",
            "Testing Normalization \n",
            "Obs Range: Original=[-0.05,1.42] -> Normalized=[-1.00,0.57]\n",
            "\n",
            "Testing Forward Pass \n",
            "Forward pass successful.\n",
            "\n",
            "Testing Gradient Clipping \n",
            "Calculated Dummy Loss: 1.8198\n",
            "Gradient norm before clipping: 1.9791\n",
            "Gradient norm after clipping (manual check): 0.5000\n",
            "Gradient clipping applied.\n",
            "\n",
            " Testing Environment: PongNoFrameskip-v4 \n",
            "\n",
            " Setting up: PongNoFrameskip-v4 \n",
            "INFO: Skipping FrameStack application (unavailable/dummy).\n",
            "\n",
            "Running tests for: PongNoFrameskip-v4\n",
            "\n",
            "Testing Normalization \n",
            "Obs Range: Original=[0.20,0.93] -> Normalized=[-0.59,0.85]\n",
            "\n",
            "Testing Forward Pass \n",
            "Forward pass successful.\n",
            "\n",
            "Testing Gradient Clipping \n",
            "Calculated Dummy Loss: 2.3427\n",
            "Gradient norm before clipping: 1.8831\n",
            "Gradient norm after clipping (manual check): 0.5000\n",
            "Gradient clipping applied.\n",
            "\n",
            " Testing Environment: HalfCheetah-v4 \n",
            "\n",
            " Setting up: HalfCheetah-v4 \n",
            "\n",
            "Running tests for: HalfCheetah-v4\n",
            "\n",
            "Testing Normalization \n",
            "Obs Range: Original=[-0.10,0.11] -> Normalized=[-0.10,0.11]\n",
            "\n",
            "Testing Forward Pass \n",
            "Forward pass successful.\n",
            "\n",
            "Testing Gradient Clipping \n",
            "Calculated Dummy Loss: 7.6106\n",
            "Gradient norm before clipping: 3.5023\n",
            "Gradient norm after clipping (manual check): 0.5000\n",
            "Gradient clipping applied.\n",
            "\n",
            " All Environment Tests Completed \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRAQoTbX4qQI",
        "outputId": "403c0ee0-1edc-4562-a16a-92ea8875ca82"
      },
      "id": "yRAQoTbX4qQI",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.9 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5BwvnHn5G9k",
        "outputId": "ae9d5243-ccee-4113-ade1-03f6e441b8eb"
      },
      "id": "n5BwvnHn5G9k",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379494 sha256=5bef9a76f1459e20859e57ad3605966aff58c6d81aac3204246de9e1c1be412f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[mujoco]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0ZOlhhD5Jlr",
        "outputId": "d18434b7-8b2f-4949-b0ff-a4bbc1be0e09"
      },
      "id": "k0ZOlhhD5Jlr",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.9.0 mujoco-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jM5nU6Nx5O4d"
      },
      "id": "jM5nU6Nx5O4d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}